{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbuJwbDWlH6V"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "print(\"Using TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Pour reproduire un comportement stable (optionnel)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "kbuJwbDWlH6V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL4ryDxBlH6X"
      },
      "source": [
        "## 1. Chargement du dataset"
      ],
      "id": "BL4ryDxBlH6X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPdyTXM1lH6X"
      },
      "source": [
        "# Chemin vers le dossier qui contient les deux sous-dossiers : 'good' et 'defect'\n",
        "DATA_DIR = \"path_to_your_mvtec_dataset\"  # À adapter\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "# Charger toutes les images d'un seul tenant\n",
        "dataset = image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',  # Pour un problème multi-classes, ici on suppose 2 classes (good, defect)\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE,\n",
        "    shuffle=True  # Mélange les données\n",
        ")\n",
        "\n",
        "# Afficher les classes détectées\n",
        "class_names = dataset.class_names\n",
        "print(\"Classes détectées :\", class_names)\n",
        "\n",
        "# Optionnel : Normalisation et mise en cache\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "def preprocess(image, label):\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # Normalisation simple\n",
        "    return image, label\n",
        "\n",
        "dataset = dataset.map(preprocess).cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "oPdyTXM1lH6X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NRj6R2GlH6Y"
      },
      "source": [
        "## 2. Définition d'un modèle simple\n",
        "On définit un modèle CNN"
      ],
      "id": "2NRj6R2GlH6Y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Em3Ej3PlH6Y"
      },
      "source": [
        "num_classes = len(class_names)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0Em3Ej3PlH6Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYnBDv0lH6Z"
      },
      "source": [
        "## 3. Entraînement du modèle"
      ],
      "id": "PnYnBDv0lH6Z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4q_eeIZlH6Z"
      },
      "source": [
        "EPOCHS = 1000  # Nombre d'époques d'entraînement\n",
        "\n",
        "history = model.fit(\n",
        "    dataset,                 # Entraînement ET test sur le même dataset\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "V4q_eeIZlH6Z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IquuiFNIlH6a"
      },
      "source": [
        "## 4. Évaluation sur le même dataset"
      ],
      "id": "IquuiFNIlH6a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFp7dBImlH6a"
      },
      "source": [
        "loss, accuracy = model.evaluate(dataset, verbose=0)\n",
        "print(f\"Loss sur le dataset complet : {loss:.4f}\")\n",
        "print(f\"Accuracy sur le dataset complet : {accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "EFp7dBImlH6a"
    }
  ],
  "metadata": {
    "name": "bad_training",
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
